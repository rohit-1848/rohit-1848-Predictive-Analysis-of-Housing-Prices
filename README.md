# ğŸ Â PredictiveÂ AnalysisÂ ofÂ HousingÂ Price

Endâ€‘toâ€‘end ML pipeline that predicts the sale price of homes in **Ames,â€¯Iowa**.  
All code and analysis live in **`PredictiveÂ AnalysisÂ ofÂ HousingÂ Price.ipynb`**.

---

## ğŸ“Â RepositoryÂ Layout
```text
â”œâ”€â”€ train.csv # 1â€¯460â€¯Ã—â€¯81 â€“ training data 
â”œâ”€â”€ test.csv # 1â€¯459â€¯Ã—â€¯80 â€“ evaluation data (no SalePrice)
â”œâ”€â”€ data_description.txt # original feature definitions
â”œâ”€â”€ Untitled.ipynbÂ (PredictiveÂ AnalysisÂ ofÂ HousingÂ Price.ipynb)
â”œâ”€â”€ Prediction.csv # output: Id,Â SalePrice
â””â”€â”€ README.md
```
---

## ğŸ”§Â Workflow Summary

| Stage | Highlights |
|-------|------------|
| **EDA** | shape checks, missingâ€‘value matrix, correlation heatâ€‘map, target distribution |
| **Preâ€‘processing** | â€¢ `SimpleImputer` (medianâ€¯/â€¯mode) â€¢ logâ€‘transform highlyâ€‘skewed numeric features â€¢ ordinal / oneâ€‘hot encoding â€¢ standard scaling for numeric |
| **ModelÂ selection** | Hyperâ€‘parameter tuning with `GridSearchCV` for:<br>Â Â â€¢ Gradientâ€¯BoostingÂ Regressor (GBR)<br>Â Â â€¢ XGBRegressor<br>Â Â â€¢ CatBoostRegressor<br>Â Â â€¢ LightGBMRegressor<br>Â Â â€¢ RandomÂ ForestÂ Regressor<br>Â Â â€¢ Ridge / Lasso / ElasticNet (baseline) |
| **Ensembles** | 1. **VotingRegressor** (`vr`) blending GBRÂ +Â XGBÂ +Â RidgeÂ (weightsÂ 2â€¯:â€¯3â€¯:â€¯1)<br>2. **StackingRegressor** (`stackreg`) with the tuned GBR,Â XGB,Â Cat,Â LGBM,Â RFR as base estimators and the `vr` voting model as metaâ€‘learner |
| **Validation** | 80â€¯/â€¯20 split â†’ metrics: **RMSE (logâ€‘scale)** & **RÂ²** |
| **Inference** | Predictions on `test.csv`, exponentiated via `np.exp`, saved to `prediction.csv` |

---

## ğŸ“ŠÂ ResultsÂ (ValidationÂ Split, logâ€‘target)

| Model | RMSE | RÂ² |
|-------|------|----|
| CatBoost | **0.1005** | 0.924 |
| GradientÂ Boosting | 0.1021 | 0.922 |
| XGBoost | 0.1039 | 0.919 |
| LightGBM | 0.1125 | 0.905 |
| RandomÂ Forest | 0.1186 | 0.894 |
| **StackedÂ Ensemble** | 0.1056 | 0.916 |

> CatBoost slightly outâ€‘performed the stacked ensemble, showing that a wellâ€‘tuned single model can still be top.

---

## ğŸš€Â QuickÂ Start

### 1ï¸âƒ£  Clone the repository and enter the project folder
```bash
git clone https://github.com/ABTG716/Housing_price.git
cd Housing_price
```

### 2ï¸âƒ£  Create and activate a virtual environment
```bash
python -m venv .venv
source .venv/bin/activate
```
#### âœ Windows: .venv\Scripts\activate

### 3ï¸âƒ£  Install dependencies
```bash
pip install -r requirements.txt
```
#### pandas, numpy, scikitâ€‘learn, xgboost, lightgbm, catboost, seaborn, matplotlib

### 4ï¸âƒ£  Launch the notebook (GUI)
```bash
jupyter notebook "Predictive Analysis of Housing Price.ipynb"
```

###     â”€â”€ or run it headless â”€â”€
```bash
jupyter nbconvert --to notebook --execute "Predictive Analysis of Housing Price.ipynb"
```

### 5ï¸âƒ£  Verify the submission file
```bash
ls -l prediction.csv
```
#### should contain:  Id,  SalePrice

---

## ğŸ“¦ Dependencies
All required packages (and specific versions) are listed in requirements.txt.
